{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.0.34)\n",
      "Requirement already satisfied: langchain-chroma in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.8.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.4)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.45 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.46)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.51)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: chromadb<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma) (0.4.24)\n",
      "Requirement already satisfied: fastapi<1,>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma) (0.110.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.0.dev0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0a0+32f93b1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.2.1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (2.4.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.7.3)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.29.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (4.11.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (3.5.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.17.3)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.24.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.59.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (4.1.2)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (29.0.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.0->langchain-chroma) (3.10.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.37.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.45->langchain-community) (1.33)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.0.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.0->langchain-chroma) (2.0.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.45->langchain-community) (2.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (2.23.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (3.2.2)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.0->langchain-chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.0->langchain-chroma) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.0->langchain-chroma) (4.24.4)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (6.8.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.63.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.24.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.24.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.45b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.45b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.45b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.45b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.45b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.45b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (68.2.2)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb<0.5.0,>=0.4.0->langchain-chroma) (2.10.1)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.38.0,>=0.37.2->fastapi<1,>=0.95.2->langchain-chroma) (4.3.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (8.1.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.0->langchain-chroma) (12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi<1,>=0.95.2->langchain-chroma) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi<1,>=0.95.2->langchain-chroma) (1.1.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (4.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (3.16.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.0->langchain-chroma) (10.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.0->langchain-chroma) (0.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community langchain-chroma sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed Data into Vector Store\n",
    "The below code assumes that the huggingface dataset has the complete chunks in one column named \"text\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slyq/miniconda3/envs/splitter/lib/python3.9/site-packages/datasets/load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "Downloading readme: 100%|██████████| 301/301 [00:00<00:00, 86.9kB/s]\n",
      "Downloading data: 100%|██████████| 692k/692k [00:00<00:00, 823kB/s]\n",
      "Generating train split: 100%|██████████| 397/397 [00:00<00:00, 11579.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from retriever import Retriever\n",
    "\n",
    "# store = Retriever(\"slyq/wdc-products-chunked\", \"combined\")\n",
    "# store = Retriever(\"slyq/code-chunked\")\n",
    "store = Retriever(\"slyq/papers-chunked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can optionally test if RAG is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\\\"MacBook Pro with Touch Bar 15-inch 256GB (2017) 256gb Space Gray\\\"  \\\"Apple MacBook Pro - 13.3\\\" Core i5 4 GB RAM 500 HDD English\\\" \\\" Apple English MD101LL/A Notebook Computers CDW.com;\\n\\\"MacBook Pro is machined from a single piece of aluminum, an engineering breakthrough that replaced many parts with just one. It's called the unibody. And the first time you pick up a MacBook Pro, you'll notice the difference it makes. The entire enclosure is thinner and lighter than other notebooks. It looks polished and refined. And it feels strong and durable - perfect for life inside (and outside) your briefcase or backpack.\\\" ;\"\n"
     ]
    }
   ],
   "source": [
    "# docs = store.query(\"How much RAM does an Apple Macbook Pro have?\")\n",
    "# print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment Eval Dataset with Documents from RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 346/346 [00:00<00:00, 96.7kB/s]\n",
      "Downloading data: 100%|██████████| 264k/264k [00:01<00:00, 263kB/s]\n",
      "Generating test split: 100%|██████████| 1976/1976 [00:00<00:00, 125031.22 examples/s]\n",
      "Map: 100%|██████████| 1976/1976 [07:29<00:00,  4.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_dataset = load_dataset(\"scottsus/papers-test\", split=\"test\")\n",
    "\n",
    "def augment(data):\n",
    "    data[\"contexts\"] = [[doc.page_content for doc in store.query(d)] for d in data[\"question\"]]\n",
    "    return data\n",
    "\n",
    "eval_dataset = raw_dataset.map(augment, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['choices', 'answer', 'question', 'contexts'],\n",
      "    num_rows: 1976\n",
      "})\n",
      "{'choices': ['A. Focused on numeric data processing only', 'B. Strong text encoding/decoding ability and reasoning', 'C. Graph visualization techniques', 'D. Purely academic data structuring'], 'answer': 'B', 'question': 'What is a significant contribution of large language models such as GPT4 and LLaMA in the field of natural language processing?', 'contexts': ['\"Large Language Models: A Survey\\\\n                              Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu\\\\n                                            Richard Socher, Xavier Amatriain, Jianfeng Gao\\\\n   Abstract\\\\u2014Large  Language  Models  (LLMs)  have  drawn  a                     that have different starting points and velocity: statistical lan-\\\\nlot  of  attention  due  to  their  strong  performance  on  a  wide            guage models, neural language models, pre-trained language\\\\nrange of natural language tasks, since the release of ChatGPT                   models and LLMs.\\\\nin November 2022. LLMs\\\\u2019 ability of general-purpose language\\\\nunderstanding and generation is acquired by training billions of                    Statisticallanguagemodels(SLMs)viewtextasasequence\\\\nmodel\\\\u2019s parameters on massive amounts of text data, as predicted                of words, and estimate the probability of text as the product\\\\nby scaling laws [1], [2]. The research area of LLMs, while very                 of  their  word  probabilities.  The  dominating  form  of  SLMs\\\\nrecent, is evolving rapidly in many different ways. In this paper,              are  Markov  chain  models  known  as  the  n-gram  models,\\\\nwe review some of the most prominent LLMs, including three                      which compute the probability of a word conditioned on its\\\\npopular LLM families (GPT, LLaMA, PaLM), and discuss their                      immediate proceeding n \\\\u2212  1  words. Since word probabilities\\\\ncharacteristics,  contributions  and  limitations.  We  also  give  an          are estimated using word and n-gram counts collected from\\\\noverview of techniques developed to build, and augment LLMs.                    text corpora, the model needs to deal with data sparsity (i.e.,\\\\nWe  then  survey  popular  datasets  prepared  for  LLM  training,              assigning zero probabilities to unseen words or n-grams) by\\\\nfine-tuning, and evaluation, review widely used LLM evaluation\\\\nmetrics, and compare the performance of several popular LLMs                    using smoothing, where some probability mass of the model\\\\non  a  set  of  representative  benchmarks.  Finally,  we  conclude             is  reserved  for  unseen  n-grams  [12].  N-gram  models  are\\\\nthe  paper  by  discussing  open  challenges  and  future  research             widely used in many NLP systems. However, these models\\\\ndirections.                                                                     are incomplete in that they cannot fully capture the diversity\\\\n                                                                                and variability of natural language due to data sparsity.\\\\n                         I.    INTRODUCTION                                         Early neural language models (NLMs) [13], [14], [15], [16]\\\\n    Language modeling is a long-standing research topic, dat-                   deal with data sparsity by mapping words to low-dimensional\\\\ning back to the 1950s with Shannon\\\\u2019s application of informa-                    continuous vectors (embedding vectors) and predict the next\\\\ntion theory to human language, where he measured how well                       word based on the aggregation of the embedding vectors of\\\\nsimple n-gram language models predict or compress natural                       its proceeding words using neural networks. The embedding\\\\nlanguage text [3]. Since then, statistical language modeling                    vectors learned by NLMs define a hidden space where the\\\\nbecame fundamental to many natural language understanding                       semantic similarity between vectors can be readily computed\\\\nand generation tasks, ranging from speech recognition, ma-                      as their distance. This opens the door to computing semantic\\\\nchine translation, to information retrieval [4], [5], [6].                      similarityofanytwoinputsregardlesstheirforms(e.g.,queries\\\\n    The recent advances on transformer-based large language                     vs. documents in Web search [17], [18], sentences in\"', '\"stronger language understanding and generation and emergent\\\\n                                                                               abilities that are not present in smaller-scale models. In what\\\\n                                                                               follows, we review three LLM families: GPT, LLaMA, and\\\\n                                                                               PaLM, as illustrated in Fig 8.\\\\n                                                                                   1) The  GPT  Family:  Generative Pre-trained Transform-\\\\n                                                                               ers  (GPT)  are  a  family  of  decoder-only  Transformer-based\\\\n                                                                               language  models,  developed  by  OpenAI.  This  family  con-\\\\n                                                                               sists of GPT-1, GPT-2, GPT-3, InstrucGPT, ChatGPT, GPT-4,\\\\n                                                                               CODEX, and WebGPT. Although early GPT models, such as\\\\n                                                                               GPT-1 and GPT-2, are open-source, recent models, such as\\\\nFig.7:High-leveloverviewofGPTpretraining,andfine-tuning                        GPT-3 and GPT-4, are close-source and can only be accessed\\\\nsteps. Courtesy of OpenAI.                                                     via APIs. GPT-1 and GPT-2 models have been discussed in\\\\n                                                                               the early PLM subsection. We start with GPT-3 below.\\\\n                                                                                   GPT-3 [56] is a pre-trained autoregressive language model\\\\n                                                                               with 175 billion parameters. GPT-3 is widely considered as\\\\n    GPT-2 [51] shows that language models are able to learn                    the first LLM in that it not only is much larger than previous\\\\nto perform specific natural language tasks without any explicit                PLMs,  but  also  for  the  first  time  demonstrates  emergent\\\\nsupervisionwhentrainedonalargeWebTextdatasetconsisting                         abilities that are not observed in previous smaller PLMs. GPT-\\\\nof millions of webpages. The GPT-2 model follows the model                     3  shows the  emergent  ability of  in-context learning,  which\\\\ndesigns of GPT-1 with a few modifications: Layer normal-                       means GPT-3 can be applied to any downstream tasks without\\\\nization is moved to the input of each sub-block, additional                    any gradient updates or fine-tuning, with tasks and few-shot\\\\nlayer normalization is added after the final self-attention block,             demonstrations specified purely via text interaction with the\\\\ninitialization is modified to account for the accumulation on                  model.  GPT-3  achieved  strong  performance  on  many  NLP\\\\nthe residual path and scaling the weights of residual layers,                  tasks, including translation, question-answering, and the cloze\\\\nvocabulary  size  is  expanded  to  50,25,  and  context  size  is             tasks, as well as several ones that require on-the-fly reasoning\\\\nincreased from 512 to 1024 tokens.                                             or domain adaptation, such as unscrambling words, using a\\\\n                                                                               novel word in a sentence, 3-digit arithmetic. Fig 9 plots the\\\\n    3) Encoder-DecoderPLMs: In[52],Raffleetal.showsthat                        performanceofGPT-3asafunctionofthenumberofexamples\\\\nalmost all NLP tasks can be cast as a sequence-to-sequence                     in in-context prompts.\\\\ngeneration task. Thus, an encoder-decoder language model, by                       CODEX [57], released by OpenAI in March 2023, is a\\\\ndesign, is a unified model in that it can per\"', '\"ay\\\\nmassive knowledge and then generate predictions through                                                    lead to sub-optimal performance on downstream tasks.\\\\nGNNs.   The latter attempts to directly employ LLMs as                                                     Compared to these non-contextualized shallow textual em-\\\\nstandalone predictors. We conduct comprehensive and sys-                                                   beddings, large language models (LLMs) present massive\\\\ntematical studies on these two pipelines under various set-                                                context-awareknowledgeandsuperiorsemanticcomprehen-\\\\ntings. From comprehensive empirical results, we make orig-                                                 sion capability through the process of pre-training on large-\\\\ninal observations and find new insights that open new pos-                                                 scale text corpora [48; 12].  This knowledge achieved from\\\\nsibilities and suggest promising directions to leverage LLMs                                               pre-traininghasledtoasurgeofrevolutionsfordownstream\\\\nforlearningongraphs. Ourcodesanddatasetsareavailable                                                       NLPtasks[85]. ExemplarssuchasChatGPTandGPT4[46],\\\\nat:   https://github.com/CurryTang/Graph-LLM                                   .                           equippedwithhundredsofbillionsofparameters,exhibitsu-\\\\n                                                                                                           perior performance [2] on numerous text-related tasks from\\\\n                                                                                                           variousdomains. Consideringtheexceptionalabilityofthese\\\\n1.   INTRODUCTION                                                                                          LLMs to process and understand textual data, a pertinent\\\\nGraphs are ubiquitous in various disciplines and applica-                                                  question arises: (1)          Can we leverage the knowledge of LLMs\\\\ntions,encompassingawiderangeofreal-worldscenarios[73].                                                     to compensate for the deficiency of contextualized knowledge\\\\nMany of these graphs have nodes that are associated with                                                   and semantic comprehension inherent in the conventional\\\\ntextattributes,resultingintheemergenceoftext-attributed                                                    GNN pipelines?             In addition to the knowledge learned via\\\\ngraphs,suchascitationgraphs[23;57]andproductgraphs[5].                                                     pre-training, recent studies suggest that LLMs present pre-\\\\nForexample,inthe           Ogbn-products             dataset[23],eachnode                                  liminarysuccessontaskswithimplicitgraphstructuressuch\\\\nrepresents a product, and its corresponding textual descrip-                                               asrecommendation[35;14],ranking[26],andmulti-hoprea-\\\\ntion is treated as the node\\\\u2019s attribute.  These graphs have                                                soning[7],inwhichLLMsareadoptedtomakethefinalpre-\\\\nseen widespread use across a myriad of domains, from social                                                dictions. Given such success, we further question: (2)                             Can\\\\nnetwork analysis [31], information retrieval [86], to a diverse                                            LLMs, beyond merely integrating with GNNs, independently\\\\nrange of natural language processing tasks [37; 76].                                                       perform predictive tasks with explicit graph structures?                                In\\\\nGiven the prevalence of text-attributed graphs (TAGs), we                                                  this paper, we aim to embark upon a preliminary investi-\\\\naimtoexplorehowtoeffectivelyhandlethesegraphs,witha\"']}\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset)\n",
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 23.20ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/slyq/papers-mcq/commit/064fe9fdb568d27b613d41910c64d9a75f5e613e', commit_message='Upload dataset', commit_description='', oid='064fe9fdb568d27b613d41910c64d9a75f5e613e', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.push_to_hub(\"slyq/papers-mcq\", \"ragged\", split=\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
