[
    {
        "question": "What unique architecture does Jamba utilize?",
        "choices": [
            "A purely Transformer-based structure",
            "A hybrid of CNN and RNN layers",
            "A hybrid of Transformer and Mamba layers",
            "A single-layer perceptron model"
        ],
        "answer": "C"
    },
    {
        "question": "What is one of the main benefits of the Jamba language model?",
        "choices": [
            "Extremely high memory requirements",
            "Ability to handle up to 256K tokens context length",
            "Low throughput and high memory footprint",
            "It uses only RNN architecture"
        ],
        "answer": "B"
    },
    {
        "question": "Which component is integrated into some layers of Jamba to enhance model capacity?",
        "choices": [
            "Convolutional layers",
            "Batch normalization",
            "Mixture-of-experts (MoE)",
            "Attention mechanisms only"
        ],
        "answer": "C"
    },
    {
        "question": "What was a major limitation of the Transformer architecture that Jamba aims to mitigate?",
        "choices": [
            "Inability to handle long contexts due to large KV cache sizes",
            "Lack of a multi-layer architecture",
            "Incapability in parallel processing",
            "Lack of recurrent neural network modules"
        ],
        "answer": "A"
    },
    {
        "question": "How does Jamba compare to traditional Transformer models in terms of resource usage?",
        "choices": [
            "Higher throughput and smaller memory footprint",
            "Lower throughput and higher resource usage",
            "Equal throughput and memory usage",
            "Only higher memory footprint"
        ],
        "answer": "A"
    },
    {
        "question": "What is the main disadvantage of RNN models compared to state space models like Mamba?",
        "choices": [
            "A) Less capable at handling long distance relationships",
            "B) More efficient training",
            "C) Can be easily parallelized across time steps",
            "D) Better at capturing hidden states"
        ],
        "answer": "A"
    },
    {
        "question": "What hybrid technology does Jamba combine to enhance model capabilities?",
        "choices": [
            "A) Mamba and Transformer layers",
            "B) Jamba and Mixtral layers",
            "C) SSM and MLP layers",
            "D) MLP and Transformer layers"
        ],
        "answer": "A"
    },
    {
        "question": "What feature of MoE layers is leveraged in Jamba to manage computational requirements?",
        "choices": [
            "A) Less number of parameters",
            "B) Flexible number of experts per MoE layer",
            "C) Same number of experts in every MoE layer",
            "D) Limited number of active parameters"
        ],
        "answer": "B"
    },
    {
        "question": "Which of the following benefits does Jamba show in comparison to Mixtral-8x7B?",
        "choices": [
            "A) Lower throughput",
            "B) Shorter context length",
            "C) Higher throughput for long contexts",
            "D) Uses more GPUs for the same context"
        ],
        "answer": "C"
    },
    {
        "question": "Where is Jamba released and under what license?",
        "choices": [
            "A) GitHub under MIT license",
            "B) Hugging Face under Apache 2.0 license",
            "C) Google Cloud under GNU GPL",
            "D) AWS Marketplace under Creative Commons"
        ],
        "answer": "B"
    },
    {
        "question": "What is the throughput advantage of Jamba compared to Mixtral-8x7B for long contexts?",
        "choices": [
            "A) 3x",
            "B) 4x",
            "C) 2x",
            "D) 5x"
        ],
        "answer": "A"
    },
    {
        "question": "How many active parameters does Jamba have?",
        "choices": [
            "A) 52B",
            "B) 12B",
            "C) 7.2B",
            "D) 6.7B"
        ],
        "answer": "B"
    },
    {
        "question": "Under which license is Jamba released?",
        "choices": [
            "A) MIT License",
            "B) GNU GPL",
            "C) Apache 2.0",
            "D) BSD"
        ],
        "answer": "C"
    },
    {
        "question": "How much KV cache memory does Jamba use for a 256K token context?",
        "choices": [
            "A) 128GB",
            "B) 32GB",
            "C) 4GB",
            "D) 8GB"
        ],
        "answer": "C"
    },
    {
        "question": "What type of model layers does Jamba incorporate?",
        "choices": [
            "A) Transformer and MoE only",
            "B) LSTM and CNN",
            "C) Transformer, Mamba, and MoE",
            "D) RNN and GRU"
        ],
        "answer": "C"
    },
    {
        "question": "What is the main purpose of increasing the ratio of Mamba layers in the Jamba architecture?",
        "choices": [
            "A: To increase the complexity and memory usage of the model",
            "B: To decrease the required memory for storing the key-value cache and improve throughput",
            "C: To reduce the number of layers in the model",
            "D: To enhance the use of positional embeddings"
        ],
        "answer": "B"
    },
    {
        "question": "What does the ratio 'a : m' in a Jamba block signify?",
        "choices": [
            "A: The number of experts per MoE layer",
            "B: The ratio of MoE layers to MLPs",
            "C: The ratio of attention layers to Mamba layers",
            "D: The memory allocation ratio for tokens"
        ],
        "answer": "C"
    },
    {
        "question": "Which normalization is applied specifically to Mamba layers to help stabilize large model training?",
        "choices": [
            "A: LayerNorm",
            "B: BatchNorm",
            "C: RMSNorm",
            "D: GroupNorm"
        ],
        "answer": "C"
    },
    {
        "question": "What impact does a larger 'n' have in the Jamba architecture?",
        "choices": [
            "A: Decreases the model capacity",
            "B: Reduces the number of top experts used",
            "C: Increases the model capacity at the cost of a larger memory footprint",
            "D: Reduces memory requirements and computational demand"
        ],
        "answer": "C"
    },
    {
        "question": "What is the total number of experts per layer in the specific Jamba implementation described for a single 80GB GPU?",
        "choices": [
            "A: 8",
            "B: 16",
            "C: 32",
            "D: 64"
        ],
        "answer": "B"
    },
    {
        "question": "What is the number of layers each Jamba block consists of?",
        "choices": [
            "A) 16",
            "B) 8",
            "C) 2",
            "D) 4"
        ],
        "answer": "B"
    },
    {
        "question": "What is the attention-to-Mamba layers ratio used in the Jamba blocks?",
        "choices": [
            "A) 1:3",
            "B) 1:5",
            "C) 1:7",
            "D) 1:9"
        ],
        "answer": "C"
    },
    {
        "question": "How many top experts are used at each token in the Jamba model?",
        "choices": [
            "A) 2",
            "B) 4",
            "C) 8",
            "D) 16"
        ],
        "answer": "A"
    },
    {
        "question": "What is the maximum context length that Jamba supports after release?",
        "choices": [
            "A) 1M tokens",
            "B) 512K tokens",
            "C) 256K tokens",
            "D) 128K tokens"
        ],
        "answer": "C"
    },
    {
        "question": "Which GPU was used to train the Jamba model?",
        "choices": [
            "A) NVIDIA H100",
            "B) A100 80GB",
            "C) NVIDIA T100",
            "D) NVIDIA V100"
        ],
        "answer": "A"
    },
    {
        "question": "By how much does Jamba's throughput increase over Mixtral for large batches?",
        "choices": [
            "A) 3x",
            "B) 2x",
            "C) 5x",
            "D) 7x"
        ],
        "answer": "A"
    },
    {
        "question": "How often is MoE used instead of a single MLP in a Jamba block?",
        "choices": [
            "A) Every layer",
            "B) Every two layers",
            "C) Every four layers",
            "D) Every eight layers"
        ],
        "answer": "B"
    },
    {
        "question": "What type of GPUs were used to train the model mentioned in the text?",
        "choices": [
            "A. NVIDIA G100",
            "B. NVIDIA H100",
            "C. AMD R100",
            "D. Intel I100"
        ],
        "answer": "B"
    },
    {
        "question": "What types of parallelism are mentioned in the text as part of the training framework?",
        "choices": [
            "A. Layer parallelism, hybrid parallelism, sequence parallelism",
            "B. Tensor parallelism, expert parallelism, sequence parallelism",
            "C. Data parallelism, layer parallelism, expert parallelism",
            "D. FSDP, tensor parallelism, hybrid parallelism"
        ],
        "answer": "B"
    },
    {
        "question": "When was the last update to Jamba's training dataset?",
        "choices": [
            "A. March 2022",
            "B. March 2023",
            "C. March 2024",
            "D. March 2025"
        ],
        "answer": "C"
    },
    {
        "question": "Which benchmark had Jamba not scored the highest compared to other models in the Reasoning category?",
        "choices": [
            "A. ARC-E",
            "B. HellaSwag",
            "C. ARC-C",
            "D. WinoGrande"
        ],
        "answer": "C"
    },
    {
        "question": "Which model has a similar number of active parameters to Jamba?",
        "choices": [
            "A. Llama-213B",
            "B. Llama-270B",
            "C. Gemma",
            "D. Mixtral"
        ],
        "answer": "D"
    },
    {
        "question": "What special architectural feature does Jamba utilize?",
        "choices": [
            "A. Layer Parallelism-Mamba",
            "B. Attention-Mamba",
            "C. Data Attention-Mamba",
            "D. Hybrid Attention-Mamba"
        ],
        "answer": "D"
    },
    {
        "question": "What is the difference in the total number of parameters between Llama-2 and Jamba?",
        "choices": [
            "A) 18 billion",
            "B) 70 billion",
            "C) 12 billion",
            "D) 58 billion"
        ],
        "answer": "A"
    },
    {
        "question": "How many active parameters does Jamba use?",
        "choices": [
            "A) 50B",
            "B) 12.9B",
            "C) 12B",
            "D) 70B"
        ],
        "answer": "C"
    },
    {
        "question": "What type of architecture does Jamba leverage?",
        "choices": [
            "A) Fully convolutional",
            "B) Hybrid Attention-Mamba",
            "C) Sparse Transformer",
            "D) Dense Neural Network"
        ],
        "answer": "B"
    },
    {
        "question": "What is the maximum context length that the released Jamba model handles?",
        "choices": [
            "A) 100K tokens",
            "B) 1M tokens",
            "C) 500K tokens",
            "D) 256K tokens"
        ],
        "answer": "D"
    },
    {
        "question": "On which evaluation did Jamba exhibit excellent performance?",
        "choices": [
            "A) Naturalistic long-context evaluation",
            "B) KV cache comparison",
            "C) Needle-in-a-haystack",
            "D) Short-context efficiency"
        ],
        "answer": "C"
    },
    {
        "question": "What is the average F1 score Jamba achieved on long-context QA benchmarks compared to Mixtral?",
        "choices": [
            "A) 0.44 vs 0.43",
            "B) 0.50 vs 0.50",
            "C) 0.40 vs 0.40",
            "D) 0.30 vs 0.29"
        ],
        "answer": "A"
    },
    {
        "question": "What model is described as not requiring explicit positional information?",
        "choices": [
            "A. Pure Mamba",
            "B. Pure Attention",
            "C. Jamba",
            "D. Hybrid Attention-Mamba"
        ],
        "answer": "C"
    },
    {
        "question": "Which normalization is necessary to stabilize training in the Mamba layers?",
        "choices": [
            "A. Layer normalization",
            "B. Special normalization",
            "C. Batch normalization",
            "D. No normalization needed"
        ],
        "answer": "B"
    },
    {
        "question": "In the tests, what was the performance result of Jamba with a ratio of 1:3 Attention-to-Mamba layers?",
        "choices": [
            "A. Worse than pure Attention",
            "B. Same as with a 1:7 ratio",
            "C. Better than pure Mamba",
            "D. Unstable in training"
        ],
        "answer": "B"
    },
    {
        "question": "What is one listed benchmark used for evaluating the models?",
        "choices": [
            "A. WinoGrande",
            "B. CodeSwag",
            "C. HellaSwag",
            "D. C4"
        ],
        "answer": "C"
    },
    {
        "question": "Which statement best describes the large-scale experiments' efficiency?",
        "choices": [
            "A. 1:7 ratio of Attention-to-Mamba layers is more compute-efficient",
            "B. 1:3 ratio of Attention-to-Mamba layers is more compute-efficient",
            "C. There is no computational difference",
            "D. Efficiency wasn't discussed"
        ],
        "answer": "A"
    },
    {
        "question": "What is the main issue with the pure Mamba model in context learning according to the text?",
        "choices": [
            "A. It adheres too closely to the required format, ignoring the context.",
            "B. It often does not follow the correct format for output.",
            "C. It performs better than the Attention model in all aspects.",
            "D. It does not utilize any transformers."
        ],
        "answer": "B"
    },
    {
        "question": "Which model showed the best performance on the IMDB dataset as mentioned?",
        "choices": [
            "A. Pure Attention",
            "B. Pure Mamba",
            "C. Attention-Mamba hybrid",
            "D. None of the above"
        ],
        "answer": "C"
    },
    {
        "question": "According to the text, what contributes to the success of the Attention-Mamba hybrid in context learning?",
        "choices": [
            "A. Integration of induction heads from transformers.",
            "B. Exclusive use of Mamba mechanisms in the model.",
            "C. Avoidance of any attention mechanism.",
            "D. Complete removal of the attention layers."
        ],
        "answer": "A"
    },
    {
        "question": "How do the three models, specifically 'No MoE' versions, differ in their training loss performance as described?",
        "choices": [
            "A. All models have the same training loss.",
            "B. Mamba has a continuously lower loss than others.",
            "C. Hybrid model has a consistently better loss profile.",
            "D. No significant differences were reported."
        ],
        "answer": "C"
    },
    {
        "question": "What is the main focus of the attention in the hybrid Attention-Mamba model?",
        "choices": [
            "A) On the Mamba layers mostly.",
            "B) On the few-shot example labels.",
            "C) On the entire dataset uniformly.",
            "D) On the first token of each sentence."
        ],
        "answer": "B"
    },
    {
        "question": "Which layers correspond to the attention layers mentioned in the hybrid model?",
        "choices": [
            "A) Layers 3, 11, 19",
            "B) Layers 1, 5, 9",
            "C) Layers 4, 12, 20",
            "D) Layers 2, 6, 10"
        ],
        "answer": "C"
    },
    {
        "question": "What does the Mixture-of-Experts (MoE) improve in the hybrid Attention-Mamba architecture?",
        "choices": [
            "A) Reduces the parameter count.",
            "B) Decreases model stability.",
            "C) Enhances the attention mechanism.",
            "D) Improves performance at a large scale."
        ],
        "answer": "D"
    },
    {
        "question": "How is MoE applied in terms of layers in the described architecture?",
        "choices": [
            "A) Every 1 layer.",
            "B) Every 2 layers.",
            "C) Every 3 layers.",
            "D) Every 4 layers."
        ],
        "answer": "B"
    },
    {
        "question": "What recent development in model strategies is mentioned that could affect future investigations?",
        "choices": [
            "A) Reducing the size of the models.",
            "B) Extracting attention-like scores from state-space models.",
            "C) Implementing MoE in classical transformers.",
            "D) Decreasing computational resources."
        ],
        "answer": "B"
    },
    {
        "question": "What did the implementation of RMSNorm achieve in the model training?",
        "choices": [
            "A) Increased the model's performance on token prediction.",
            "B) Reduced the dimensionality of the model layers.",
            "C) Stabilized training and prevented loss spikes.",
            "D) Enhanced MoE integration with state-space models."
        ],
        "answer": "C"
    },
    {
        "question": "According to table 8, what can be inferred about the requirement for explicit positional information in Jamba architecture?",
        "choices": [
            "A) Explicit positional information significantly improves performance.",
            "B) Without explicit positional information, performance drastically drops.",
            "C) Implicit positional information from Mamba layers may be sufficient.",
            "D) Adding explicit positional information had no effect."
        ],
        "answer": "C"
    },
    {
        "question": "What does Jamba in the text represent?",
        "choices": [
            "A brand of juice",
            "A novel architecture combining Attention and Mamba layers",
            "A statistical method",
            "A type of data model"
        ],
        "answer": "B"
    },
    {
        "question": "What is the purpose of MoE modules in Jamba's architecture?",
        "choices": [
            "To reduce computation time",
            "To provide encryption",
            "To enhance state-of-the-art performance",
            "To handle peripheral device management"
        ],
        "answer": "C"
    },
    {
        "question": "Which component of Jamba accommodates long context lengths?",
        "choices": [
            "Its 52B total available parameters",
            "Its multi-layer encoding",
            "Its explicit positional encodings",
            "Its flexible architecture"
        ],
        "answer": "A"
    },
    {
        "question": "According to the text, what is the maximum context length the largest model of Jamba can support?",
        "choices": [
            "52K tokens",
            "12K tokens",
            "256K tokens",
            "100K tokens"
        ],
        "answer": "C"
    },
    {
        "question": "What is the purpose of releasing smaller-scale training run model checkpoints mentioned in the text?",
        "choices": [
            "For educational purposes",
            "For testing network infrastructure",
            "To facilitate future research",
            "For financial profit"
        ],
        "answer": "C"
    },
    {
        "question": "Which of the references in the text is associated with discussing standard evaluations for long context language models?",
        "choices": [
            "[2] Chenxin An et al., L-Eval",
            "[5] Eunsol Choi et al., QuAC",
            "[4] Mark Chen et al., Evaluating large language models",
            "[3] Yonatan Bisk et al., PIQA"
        ],
        "answer": "A"
    },
    {
        "question": "What is the primary subject of the paper titled 'Think you have solved question answering? Try ARC, the AI2 reasoning challenge' by Peter Clark and others?",
        "choices": [
            "A. Development of a new AI reasoning challenge",
            "B. Overview of natural language processing technologies",
            "C. Data compression algorithms",
            "D. Neural network architectures for image recognition"
        ],
        "answer": "A"
    },
    {
        "question": "Which publication focuses on training verifiers for solving math word problems?",
        "choices": [
            "A. Switch transformers",
            "B. Training verifiers to solve math word problems",
            "C. Mamba: Linear-time sequence modeling",
            "D. Hungry hungry hippos: Towards language modeling"
        ],
        "answer": "B"
    },
    {
        "question": "In which year was the algorithm for data compression by Philip Gage published?",
        "choices": [
            "A. 1994",
            "B. 2021",
            "C. 2018",
            "D. 2023"
        ],
        "answer": "A"
    },
    {
        "question": "What innovation is discussed in 'Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity'?",
        "choices": [
            "A. Language modeling with state space models",
            "B. Scaling transformer models using a novel sparsity technique",
            "C. Data compression techniques",
            "D. Enhancements in multi-head attention mechanisms"
        ],
        "answer": "B"
    },
    {
        "question": "Which conference did the publication 'CUAD: An expert-annotated NLP dataset for legal contract review' appear in?",
        "choices": [
            "A. EMNLP 2021",
            "B. ICL 2020",
            "C. NeurIPS 2021",
            "D. ICLR 2021"
        ],
        "answer": "C"
    },
    {
        "question": "What is the main focus of the NarrativeQA reading comprehension challenge?",
        "choices": [
            "A. Comparing narrative styles across cultures",
            "B. Designing better natural language generation models",
            "C. Benchmarking for question-answering research",
            "D. Understanding narrative forms in textual data"
        ],
        "answer": "D"
    },
    {
        "question": "In which year was the paper concerning 'Natural questions: a benchmark for question answering research' published?",
        "choices": [
            "A. 2015",
            "B. 2017",
            "C. 2018",
            "D. 2019"
        ],
        "answer": "D"
    },
    {
        "question": "Which conference was the research from Stephanie Lin, Jacob Hilton, and Owain Evans presented?",
        "choices": [
            "A. ACL Long Papers 2022",
            "B. ICML 2022",
            "C. NeurIPS 2021",
            "D. EMNLP 2020"
        ],
        "answer": "A"
    },
    {
        "question": "What was the primary subject of the publication written by Andrew Maas and colleagues in 2011?",
        "choices": [
            "A. Tokenization in NLP",
            "B. Speech recognition systems",
            "C. Sentiment analysis using word vectors",
            "D. Truthful QA frameworks"
        ],
        "answer": "C"
    },
    {
        "question": "Which of these describes the term 'MoE-Mamba' found in a 2024 publication?",
        "choices": [
            "A. A type of sentiment analysis software",
            "B. A benchmark in question answering",
            "C. A novel state space model with mixture of experts",
            "D. An algorithm for synthetic data generation"
        ],
        "answer": "C"
    },
    {
        "question": "What is a primary focus in the 2023 published work 'Hyena hierarchy: Towards larger convolutional language models'?",
        "choices": [
            "A. Improving small-scale language models",
            "B. Transitioning to convolutional approaches in language models",
            "C. Development of a new speech recognition framework",
            "D. Enhancing the efficiency of in-context learning"
        ],
        "answer": "B"
    },
    {
        "question": "What is the main focus of the paper by George Saon, Ankit Gupta, and Xiaodong Cui presented in ICASSP 2023?",
        "choices": [
            "A) Improvements in state space augmented transformers for speech recognition",
            "B) Developments in neural machine translation",
            "C) Enhancing transformer models with GLU variants",
            "D) Rotary position embedding in enhanced transformers"
        ],
        "answer": "A"
    },
    {
        "question": "In which year was the paper 'Neural machine translation of rare words with subword units' by Rico Sennrich, Barry Haddow, and Alexandra Birch published?",
        "choices": [
            "A) 2016",
            "B) 2017",
            "C) 2020",
            "D) 2023"
        ],
        "answer": "A"
    },
    {
        "question": "What is the innovative aspect discussed by Noam Shazeer and his colleagues in the 2017 publication at the International Conference on Learning Representations?",
        "choices": [
            "A) Advances in neural machine translation",
            "B) The sparsely-gated mixture-of-experts layer",
            "C) Root mean square layer normalization",
            "D) Gemini research and technology for open models"
        ],
        "answer": "B"
    },
    {
        "question": "Which publication specifically addresses advancements in the field of speech and signal processing presented at ICASSP?",
        "choices": [
            "A) Diagonal state space augmented transformers for speech recognition",
            "B) Neural machine translation of rare words with subword units",
            "C) Glu variants improve transformer",
            "D) Rotary position embedding in enhanced transformers"
        ],
        "answer": "A"
    }
]